{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T15:18:57.298363Z",
     "start_time": "2021-09-23T15:18:45.256226Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"Loading/\")\n",
    "sys.path.insert(0,\"Preprocessing/\")\n",
    "sys.path.insert(0,\"Modeling/\")\n",
    "sys.path.insert(0,\"Evaluation/\")\n",
    "sys.path.insert(0,\"Interpretability/\")\n",
    "sys.path.insert(0,\"Monitoring/\")\n",
    "sys.path.insert(0,\"Utils/\")\n",
    "\n",
    "\n",
    "import loading\n",
    "import preprocessing\n",
    "import modeling\n",
    "import evaluation\n",
    "import interpretability\n",
    "import monitoring \n",
    "import utils as u\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Package allowing to reload a package (or a script), without having to reload the whole notebook\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T15:18:57.389134Z",
     "start_time": "2021-09-23T15:18:57.343238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we find the parameters needed for the run. You have to launch this cell only if you do not launch the main script.\n",
    "\n",
    "path_conf =\"../params/conf/conf.json\"\n",
    "\n",
    "# path_conf ='../conf/conf.json'\n",
    "conf = json.load(open(path_conf, 'r'))\n",
    "\n",
    "path_log = conf['path_log'] # \"../log/my_log_file.txt\"\n",
    "log_level = conf['log_level'] # \"DEBUG\"\n",
    "\n",
    "# Be careful to launch the logger only once, otherwise each lines will be duplicated\n",
    "logger = u.my_get_logger(path_log, log_level, my_name=\"main_logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:37:24.458208Z",
     "start_time": "2021-09-23T16:37:24.443575Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reload of the conf file (useful when you do modifications)\n",
    "conf = json.load(open(path_conf, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:37:25.583445Z",
     "start_time": "2021-09-23T16:37:24.787453Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(loading)\n",
    "\n",
    "#Reading of the dataset selected in the conf file\n",
    "df = loading.read_csv_from_name(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:37:50.024396Z",
     "start_time": "2021-09-23T16:37:25.595320Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)\n",
    "importlib.reload(u)\n",
    "\n",
    "#Preprocessing of the selected dataset\n",
    "df_preprocessed, X_columns, y_column = preprocessing.main_preprocessing_from_name(df,conf)\n",
    "\n",
    "#Writting of the preprocessed dataset\n",
    "loading.write_preprocessed_csv_from_name(df_preprocessed,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:37:50.386726Z",
     "start_time": "2021-09-23T16:37:50.037984Z"
    }
   },
   "outputs": [],
   "source": [
    "#Basic Splitting between train and test\n",
    "X_train, X_test, y_train, y_test = preprocessing.basic_split( df_preprocessed , 0.25 , X_columns, y_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:39:43.363865Z",
     "start_time": "2021-09-23T16:37:50.399444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(modeling)\n",
    "#Modelisation using the model selected in the conf file\n",
    "clf, best_params = modeling.main_modeling_from_name(X_train,y_train,conf)\n",
    "#Saving the model\n",
    "u.save_model(clf, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:39:44.858056Z",
     "start_time": "2021-09-23T16:39:43.367793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Outputs/Models/drift_random_forest.sav\n"
     ]
    }
   ],
   "source": [
    "#Independent step from the other, we reload what we need:\n",
    "importlib.reload(loading)\n",
    "importlib.reload(u)\n",
    "\n",
    "#Loading of the model\n",
    "clf = u.load_model(conf)\n",
    "#Loading of the preprocessed dataset\n",
    "df = loading.load_preprocessed_csv_from_name(conf)\n",
    "\n",
    "#Basic Splitting:\n",
    "y_column = u.get_y_column_from_conf(conf)\n",
    "X_columns = [x for x in df.columns if x != y_column ]\n",
    "X_train, X_test, y_train, y_test = preprocessing.basic_split( df , 0.25 , X_columns, y_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T16:39:47.981690Z",
     "start_time": "2021-09-23T16:39:44.862141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.8074476543864298,\n",
       " 'accuracy': 0.76752,\n",
       " 'recall': 0.8628478368618565,\n",
       " 'precision': 0.7587323329805118,\n",
       " 'confusion_matrix': {'tn': 7002, 'fp': 3875, 'fn': 1937, 'tp': 12186}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(evaluation)\n",
    "#Computing metrics\n",
    "dict_metrics = evaluation.main_evaluation(clf, X_train, y_train, X_test, y_test)\n",
    "dict_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading what you need ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST AUTRES - TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T11:44:19.871815Z",
     "start_time": "2021-09-22T11:44:18.660471Z"
    }
   },
   "outputs": [],
   "source": [
    "#vÃ©rif Drift: \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, accuracy_score,confusion_matrix\n",
    "import numpy as np\n",
    "params = {'objective': 'binary'}\n",
    "\n",
    "y_train = df['class'].loc[:10000]\n",
    "X_train =df[[x for x in df.columns if x != \"class\"]].loc[:10000]\n",
    "\n",
    "dftrainLGB = lgb.Dataset(data=X_train, label=y_train, feature_name=list(X_train))\n",
    "\n",
    "clf = lgb.train(\n",
    "    params,\n",
    "    dftrainLGB,\n",
    "    num_boost_round=100    )\n",
    "\n",
    "for i in np.arange(10): \n",
    "    sub_x_test = df[[x for x in df.columns if x != \"class\"]].loc[i*10000:(i+1)*10000]\n",
    "    sub_y_test =  df['class'].loc[i*10000:(i+1)*10000]\n",
    "\n",
    "    y_test_pred = np.array([clf.predict(sub_x_test) >= 0.5], dtype=np.float32)[0]\n",
    "    print(i,f1_score(sub_y_test, y_test_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
